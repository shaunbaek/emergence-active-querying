# Emergence: Overcoming Privileged Information Bias in Asymmetric Embodied Agents via Active Querying

**Research Artifact for Paper Submission**

## Abstract

Large Language Models (LLMs) suffer from "symbol grounding" failures in embodied environments, particularly where information is asymmetrically distributed. We investigate the "Privileged Information Bias" (or Curse of Knowledge), where a knowledgeable "Leader" agent fails to guide a sensor-limited "Follower" due to a lack of Theory of Mind. We propose a novel Asymmetric Assistive Reasoning framework within AI2-THOR to quantify this phenomenon. Our experiments reveal a significant "Success Gap": while the Leader agent successfully perceives the target in 35.0% of episodes, the collaborative team succeeds only 17.0% of the time. This implies that nearly half of all feasible plans fail during communication. We demonstrate that a "Pull-based" communication protocol (active querying by the follower) is significantly more robust than "Push-based" instruction, with successful episodes featuring 2x the frequency of clarification requests. This research contributes to the broader fields of assistive robotics, autonomous systems and collaborative human-AI and AI-AI systems.

## Repository Contents

This repository contains the complete experimental framework and raw data for reproducing our results:

- **Core experiment code** - Main runners and agent policies
- **Task definitions** - 100 navigation tasks used in all experiments  
- **Raw experimental data** - 300 individual task logs (100 per condition)
- **Configuration** - Dependencies and setup instructions

## Setup Instructions

### 1. Create a Virtual Environment

First, create a Python virtual environment to isolate project dependencies:

```bash
python3 -m venv .venv
```

> **Note:** You can rename `.venv` to any name you prefer (e.g., `venv`, `env`, etc.)

### 2. Activate the Virtual Environment

Activate the virtual environment using:

```bash
source .venv/bin/activate
```

> **Note:** On Windows, use `.venv\Scripts\activate` instead

### 3. Install Dependencies

Once the virtual environment is activated, upgrade pip and install the required packages:

```bash
# Upgrade pip to the latest version
pip install --upgrade pip

# Install all project dependencies from requirements.txt
pip install -r requirements.txt
```

**Required packages:**
- `ai2thor` - AI2-THOR 3D simulator
- `matplotlib` - Plotting and visualization
- `google-generativeai` - Google Gemini API
- `python-dotenv` - Environment variable management

### 4. Configure Environment Variables

Create a `.env` file in the project root directory by copying the example file:

```bash
cp .env.example .env
```

Then edit the `.env` file and add your Gemini API key:

```
GEMINI_API_KEY=your_actual_api_key_here
```

> **Note:** You can get your Gemini API key from [Google AI Studio](https://makersuite.google.com/app/apikey)

### 5. Running the Project

After completing the setup, you can run navigation experiments with different policies and task sets.

## Running Navigation Experiments

### Experimental Conditions

The repository includes three agent policies corresponding to our experimental conditions:

- **`baseline`** (Leader) - Single agent with full visibility and rich perception
  - Represents the privileged leader with complete environmental information
  - Success Rate: 35.0% (establishes upper bound for collaborative success)
  
- **`handicapped`** (Follower) - Single agent with limited perception (only sees nearby objects)
  - Represents the sensor-limited follower requiring guidance
  - Success Rate: 9.0% (establishes need for collaboration)
  
- **`two_agent`** (Collaborative) - Leader (full visibility) + Follower (limited perception) with communication
  - Tests collaborative navigation with asymmetric information
  - Success Rate: 17.0% (reveals the Success Gap phenomenon)

### Task Set

The experiments use a standardized set of 100 navigation tasks (`tasks_full_100.json`):
- **Reproducible**: Generated with fixed seed (42) for consistency across runs
- **Balanced**: Distributed across all room types (kitchen, living room, bedroom, bathroom)
- **Shared**: Same task set used for all three experimental conditions
- Generated by `generate_task_list_100()` in `tasks_utils.py`

### Running Experiments

**Reproducing paper results** - Run all three conditions with the standardized task set:

```bash
# Baseline (Leader) - 100 tasks, 30 steps max
python3 loop.py --policy baseline --taskset full100 --max_steps 30

# Handicapped (Follower) - 100 tasks, 30 steps max
python3 loop.py --policy handicapped --taskset full100 --max_steps 30

# Two-Agent (Collaborative) - 100 tasks, 30 steps max
python3 loop.py --policy two_agent --taskset full100 --max_steps 30
```

**Resume interrupted runs:**

```bash
python3 loop.py --policy baseline --taskset full100 --max_steps 30 --resume
python3 loop.py --policy handicapped --taskset full100 --max_steps 30 --resume
python3 loop.py --policy two_agent --taskset full100 --max_steps 30 --resume
```

**Development/Testing:**

```bash
# Quick test with 20 dev tasks
python3 loop.py --policy baseline --taskset dev20 --max_steps 30
```

### Command-line Arguments

- `--policy` - Choose agent policy: `baseline`, `handicapped`, or `two_agent` (default: `baseline`)
- `--taskset` - Choose task set: `dev20`, `full100`, or `custom100` (default: `full100`)
  - **`full100` is RECOMMENDED** - 100 reproducible tasks from `tasks_utils.py`
- `--max_steps` - Maximum steps per episode (default: `50`)
- `--resume` - Skip tasks that already have log files (useful for resuming interrupted runs)

> **Important**: When using `--resume`, always specify `--max_steps` to ensure consistency with your previous run. The default is 50 steps if not specified.

### Results

Episode logs are saved to:
- `logs/baseline/` - Baseline policy results
- `logs/handicapped/` - Handicapped policy results
- `logs/two_agent/` - Two-agent policy results

Each task produces a JSON file named `task_<id>.json` containing:
- Task metadata (scene, room type, target object, goal)
- Episode duration
- Step-by-step trajectory with actions, positions, and distances
- Agent memory and communication logs (for two-agent policy)

### Generating Task Files

To regenerate the task files:

```bash
# Generate all standard task sets
python tasks_utils.py

# Generate custom 100 tasks
python tasks_custom_100.py
```

### Run All Experiments at Once

For convenience, use the provided script to run all 100 tasks across all three policies sequentially:

```bash
# Run all experiments
./run_all_experiments.sh

# Run with resume support (skip completed tasks)
./run_all_experiments.sh --resume

# Customize max steps per task
./run_all_experiments.sh --max-steps 75
```

This script will:
1. Check prerequisites (virtual environment, .env file)
2. Generate task files if needed
3. Run baseline policy (100 tasks)
4. Run handicapped policy (100 tasks)
5. Run two-agent policy (100 tasks)
6. Report completion status

**Note**: Running all experiments takes approximately 3-7.5 hours total.

### Run Partial Tasks for Two-Agent Policy

For testing or running specific subsets of tasks with the two-agent policy, use the partial task runner:

```bash
# Run tasks 1-20
python3 run_partial_two_agent.py --start 1 --end 20 --max_steps 30

# Run specific tasks by ID
python3 run_partial_two_agent.py --tasks 1,5,10,15,20 --max_steps 30

# Resume interrupted run (skip completed tasks)
python3 run_partial_two_agent.py --start 1 --end 50 --max_steps 30 --resume

# Run last 10 tasks
python3 run_partial_two_agent.py --start 91 --end 100 --max_steps 30
```

**Features:**
- **Range mode**: Use `--start` and `--end` to run a continuous range of tasks
- **Specific tasks mode**: Use `--tasks` with comma-separated IDs to run specific tasks
- **Resume support**: Use `--resume` to skip already completed tasks
- **Progress tracking**: Shows completed/skipped counts and task distribution

## Troubleshooting

### Missing Module Errors

If you encounter `ModuleNotFoundError`, ensure all dependencies are installed:

```bash
pip install -r requirements.txt
```

### Missing env.py

The project requires `env.py` which provides the `ThorEnv` wrapper for AI2-THOR. This file should be present in the project root.

### API Key Issues

Verify your Gemini API key is properly configured:

```bash
cat .env | grep GEMINI_API_KEY
```

### AI2-THOR Display Issues

If running on a headless server or encountering display issues, AI2-THOR may need additional configuration. Refer to the [AI2-THOR documentation](https://ai2thor.allenai.org/ithor/documentation/) for details.

## Deactivating the Virtual Environment

When you're done working on the project, you can deactivate the virtual environment:

```bash
deactivate
```
